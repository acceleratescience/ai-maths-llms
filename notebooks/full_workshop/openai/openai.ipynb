{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the OpenAI API\n",
    "\n",
    "In this section we will cover the basics of using the OpenAI API, including:\n",
    "- Chat Completions\n",
    "- Streaming\n",
    "- Vision input\n",
    "\n",
    "The beauty of the OpenAI API is that is very simple to use.\n",
    "\n",
    "In your environment you should have a file called `.env` with the following:\n",
    "\n",
    "```bash\n",
    "OPENAI_API_KEY=\"sk-proj-1234567890\"\n",
    "```\n",
    "\n",
    "We will give you this key in the workshop. __The key will be deactivated after the workshop!__\n",
    "\n",
    "You can then grab the key using python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import os\n",
    "from rich import print as rprint # for making fancy outputs\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling a model is simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steel blade in moonlight,  \n",
      "Silent paws tread on the grass—  \n",
      "Honor in each leap.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are Matsuo Basho, the great Japanese haiku poet.\"\n",
    "user_query = \"Can you give me a haiku about a Samurai cat.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_query},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purrfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API offers a number of _endpoints_ that allow you to interact with the models. The main one that we will cover here is the `/chat/completions` endpoint. This endpoint allows you to interact with the model in a conversational manner.\n",
    "\n",
    "Only 2 arguments are actually required for this endpoint:\n",
    "\n",
    "- `model: str` The model to use. For OpenAI, this includes:\n",
    "    - `'gpt-3.5-turbo'`\n",
    "\n",
    "    - `'gpt-4'`\n",
    "\n",
    "    - `'gpt-4o'`\n",
    "\n",
    "    - `'gpt-4o-mini'`\n",
    "    \n",
    "    - Any fine-tuned versions of these models.\n",
    "    \n",
    "    - Many specific versions of the above models.\n",
    "\n",
    "- `messages: list` A list of messages that the model should use to generate a response. Each entry in the list of messages comes in the form:\n",
    "\n",
    "```python\n",
    "{\"role\": \"<role>\", \"content\": \"<content>\", \"name\": \"<name>\"}\n",
    "```\n",
    "\n",
    "Where `<role>` can take one of the following forms:\n",
    "\n",
    "- `'system'` This is a system level prompt, designed to guide the conversation. For example: \n",
    "\n",
    "_\"You are a customer service bot.\"_\n",
    "\n",
    "- `'user'` This is direct input from the user. For example: \n",
    "\n",
    "_\"How do I reset my password?\"_\n",
    "\n",
    "- `'assistant'` This is the response from the model. For example:\n",
    "\n",
    "_\"To reset your password, please visit our website and click on the 'Forgot Password' link.\"_\n",
    "\n",
    "So all of this fed into one message list would look like this:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a customer service bot.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I reset my password?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"To reset your password, please visit our website and click on the 'Forgot Password' link.\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional arguments\n",
    "The `/chat/completions` endpoint also accepts a number of additional arguments that can be used to alter the response. These include (arguments are listed with their default values if applicable):\n",
    "\n",
    "- `max_tokens: int` The maximum number of tokens to generate in the response. Important to stop the model from generating too much text and racking up a huge bill.\n",
    "\n",
    "- `n: int = 1` The number of completions to generate. This is useful when you want to generate multiple completions and select the best one. You'll be charged for the _**total**_ number of tokens generated across all completions, so be careful with setting this too high.\n",
    "\n",
    "- `temperature: float = 1.0` The temperature of the model, ranging from 0.0 to 2. Use low values for deterministic responses, and high values for more creative responses.\n",
    "\n",
    "- `top_p: float = 1.0` The probability of sampling from the top `p` tokens. This is useful for controlling the diversity of the responses. Setting this to a higher values means the model is more likely to sample from a wider range of tokens.\n",
    "\n",
    "- `logprobs: bool = False` Whether to return the log probabilities of the tokens generated. This is useful when you want to understand how the model is making decisions.\n",
    "\n",
    "- `logit_bias: dict` A dictionary of logit biases to apply to the tokens. This is useful when you want to guide the model towards generating certain types of responses.\n",
    "\n",
    "- `response_format: str` The format of the response. We will cover this later...\n",
    "\n",
    "- `stream: bool = False` Whether to stream the response back to the client. This is useful when you want to get the response in real-time. Nobody likes to sit and wait for a response. Seeing the text generated as and when it is ready is a much better user experience.\n",
    "\n",
    "For a full list of arguments, check out the [OpenAI API documentation](https://platform.openai.com/docs/api-reference/chat/create)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used model `gpt-4o-mini`, but there are a range of models available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai')\n",
      "Model(id='gpt-4', created=1687882411, object='model', owned_by='openai')\n",
      "Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai')\n",
      "Model(id='chatgpt-image-latest', created=1765925279, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-tts-2025-03-20', created=1765610731, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-tts-2025-12-15', created=1765610837, object='model', owned_by='system')\n",
      "Model(id='gpt-realtime-mini-2025-12-15', created=1765612007, object='model', owned_by='system')\n",
      "Model(id='gpt-audio-mini-2025-12-15', created=1765760008, object='model', owned_by='system')\n",
      "Model(id='davinci-002', created=1692634301, object='model', owned_by='system')\n",
      "Model(id='babbage-002', created=1692634615, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system')\n",
      "Model(id='dall-e-3', created=1698785189, object='model', owned_by='system')\n",
      "Model(id='dall-e-2', created=1698798177, object='model', owned_by='system')\n",
      "Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system')\n",
      "Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system')\n",
      "Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system')\n",
      "Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system')\n",
      "Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system')\n",
      "Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system')\n",
      "Model(id='gpt-4-0125-preview', created=1706037612, object='model', owned_by='system')\n",
      "Model(id='gpt-4-turbo-preview', created=1706037777, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system')\n",
      "Model(id='gpt-4-turbo', created=1712361441, object='model', owned_by='system')\n",
      "Model(id='gpt-4-turbo-2024-04-09', created=1712601677, object='model', owned_by='system')\n",
      "Model(id='gpt-4o', created=1715367049, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system')\n",
      "Model(id='chatgpt-4o-latest', created=1723515131, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-audio-preview', created=1727460443, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-realtime-preview', created=1727659998, object='model', owned_by='system')\n",
      "Model(id='omni-moderation-latest', created=1731689265, object='model', owned_by='system')\n",
      "Model(id='omni-moderation-2024-09-26', created=1732734466, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-realtime-preview-2024-12-17', created=1733945430, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-audio-preview-2024-12-17', created=1734034239, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-realtime-preview-2024-12-17', created=1734112601, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-audio-preview-2024-12-17', created=1734115920, object='model', owned_by='system')\n",
      "Model(id='o1-2024-12-17', created=1734326976, object='model', owned_by='system')\n",
      "Model(id='o1', created=1734375816, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-realtime-preview', created=1734387380, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-audio-preview', created=1734387424, object='model', owned_by='system')\n",
      "Model(id='o3-mini', created=1737146383, object='model', owned_by='system')\n",
      "Model(id='o3-mini-2025-01-31', created=1738010200, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-2024-11-20', created=1739331543, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-search-preview-2025-03-11', created=1741388170, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-search-preview', created=1741388720, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-search-preview-2025-03-11', created=1741390858, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-search-preview', created=1741391161, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-transcribe', created=1742068463, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-transcribe', created=1742068596, object='model', owned_by='system')\n",
      "Model(id='o1-pro-2025-03-19', created=1742251504, object='model', owned_by='system')\n",
      "Model(id='o1-pro', created=1742251791, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-tts', created=1742403959, object='model', owned_by='system')\n",
      "Model(id='o3-2025-04-16', created=1744133301, object='model', owned_by='system')\n",
      "Model(id='o4-mini-2025-04-16', created=1744133506, object='model', owned_by='system')\n",
      "Model(id='o3', created=1744225308, object='model', owned_by='system')\n",
      "Model(id='o4-mini', created=1744225351, object='model', owned_by='system')\n",
      "Model(id='gpt-4.1-2025-04-14', created=1744315746, object='model', owned_by='system')\n",
      "Model(id='gpt-4.1', created=1744316542, object='model', owned_by='system')\n",
      "Model(id='gpt-4.1-mini-2025-04-14', created=1744317547, object='model', owned_by='system')\n",
      "Model(id='gpt-4.1-mini', created=1744318173, object='model', owned_by='system')\n",
      "Model(id='gpt-4.1-nano-2025-04-14', created=1744321025, object='model', owned_by='system')\n",
      "Model(id='gpt-4.1-nano', created=1744321707, object='model', owned_by='system')\n",
      "Model(id='gpt-image-1', created=1745517030, object='model', owned_by='system')\n",
      "Model(id='codex-mini-latest', created=1746673257, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-realtime-preview-2025-06-03', created=1748907838, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-audio-preview-2025-06-03', created=1748908498, object='model', owned_by='system')\n",
      "Model(id='o4-mini-deep-research', created=1749685485, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-transcribe-diarize', created=1750798887, object='model', owned_by='system')\n",
      "Model(id='o4-mini-deep-research-2025-06-26', created=1750866121, object='model', owned_by='system')\n",
      "Model(id='gpt-5-chat-latest', created=1754073306, object='model', owned_by='system')\n",
      "Model(id='gpt-5-2025-08-07', created=1754075360, object='model', owned_by='system')\n",
      "Model(id='gpt-5', created=1754425777, object='model', owned_by='system')\n",
      "Model(id='gpt-5-mini-2025-08-07', created=1754425867, object='model', owned_by='system')\n",
      "Model(id='gpt-5-mini', created=1754425928, object='model', owned_by='system')\n",
      "Model(id='gpt-5-nano-2025-08-07', created=1754426303, object='model', owned_by='system')\n",
      "Model(id='gpt-5-nano', created=1754426384, object='model', owned_by='system')\n",
      "Model(id='gpt-audio-2025-08-28', created=1756256146, object='model', owned_by='system')\n",
      "Model(id='gpt-realtime', created=1756271701, object='model', owned_by='system')\n",
      "Model(id='gpt-realtime-2025-08-28', created=1756271773, object='model', owned_by='system')\n",
      "Model(id='gpt-audio', created=1756339249, object='model', owned_by='system')\n",
      "Model(id='gpt-5-codex', created=1757527818, object='model', owned_by='system')\n",
      "Model(id='gpt-image-1-mini', created=1758845821, object='model', owned_by='system')\n",
      "Model(id='gpt-5-pro-2025-10-06', created=1759469707, object='model', owned_by='system')\n",
      "Model(id='gpt-5-pro', created=1759469822, object='model', owned_by='system')\n",
      "Model(id='gpt-audio-mini', created=1759512027, object='model', owned_by='system')\n",
      "Model(id='gpt-audio-mini-2025-10-06', created=1759512137, object='model', owned_by='system')\n",
      "Model(id='gpt-5-search-api', created=1759514629, object='model', owned_by='system')\n",
      "Model(id='gpt-realtime-mini', created=1759517133, object='model', owned_by='system')\n",
      "Model(id='gpt-realtime-mini-2025-10-06', created=1759517175, object='model', owned_by='system')\n",
      "Model(id='sora-2', created=1759708615, object='model', owned_by='system')\n",
      "Model(id='sora-2-pro', created=1759708663, object='model', owned_by='system')\n",
      "Model(id='gpt-5-search-api-2025-10-14', created=1760043960, object='model', owned_by='system')\n",
      "Model(id='gpt-5.1-chat-latest', created=1762547951, object='model', owned_by='system')\n",
      "Model(id='gpt-5.1-2025-11-13', created=1762800353, object='model', owned_by='system')\n",
      "Model(id='gpt-5.1', created=1762800673, object='model', owned_by='system')\n",
      "Model(id='gpt-5.1-codex', created=1762988221, object='model', owned_by='system')\n",
      "Model(id='gpt-5.1-codex-mini', created=1763007109, object='model', owned_by='system')\n",
      "Model(id='gpt-5.1-codex-max', created=1763671532, object='model', owned_by='system')\n",
      "Model(id='gpt-image-1.5', created=1764030620, object='model', owned_by='system')\n",
      "Model(id='gpt-5.2-2025-12-11', created=1765313028, object='model', owned_by='system')\n",
      "Model(id='gpt-5.2', created=1765313051, object='model', owned_by='system')\n",
      "Model(id='gpt-5.2-pro-2025-12-11', created=1765343959, object='model', owned_by='system')\n",
      "Model(id='gpt-5.2-pro', created=1765343983, object='model', owned_by='system')\n",
      "Model(id='gpt-5.2-chat-latest', created=1765344352, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-transcribe-2025-12-15', created=1765610407, object='model', owned_by='system')\n",
      "Model(id='gpt-4o-mini-transcribe-2025-03-20', created=1765610545, object='model', owned_by='system')\n",
      "Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal')\n",
      "Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal')\n",
      "Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal')\n",
      "Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal')\n",
      "Model(id='ft:gpt-4o-mini-2024-07-18:accelerate-science:medical-memo-5-epochs:ApzySf8o:ckpt-step-375', created=1736956320, object='model', owned_by='cambridge-39')\n",
      "Model(id='ft:gpt-4o-mini-2024-07-18:accelerate-science:medical-memo-5-epochs:ApzySQIf:ckpt-step-500', created=1736956320, object='model', owned_by='cambridge-39')\n",
      "Model(id='ft:gpt-4o-mini-2024-07-18:accelerate-science:medical-memo-5-epochs:ApzySybZ', created=1736956320, object='model', owned_by='cambridge-39')\n"
     ]
    }
   ],
   "source": [
    "for model in client.models.list():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of our examples, we will stick with `gpt-5-nano` and `gpt-5-mini`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The response object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the `response` object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletion</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-CvnXdHCse6ar7hr12oDul0CnzDjI3'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Choice</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatCompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'In moonlit gate, swift paws,\\nsteel gleams, a whiskered whisper—\\nvirtue purrs through </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">night.'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">refusal</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">annotations</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">audio</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1767891169</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-5-nano-2025-08-07'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionUsage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">total_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">70</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">completion_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionTokensDetails</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">accepted_prediction_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">reasoning_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">rejected_prediction_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PromptTokensDetails</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">audio_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">cached_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1;35mChatCompletion\u001B[0m\u001B[1m(\u001B[0m\n",
       "    \u001B[33mid\u001B[0m=\u001B[32m'chatcmpl-CvnXdHCse6ar7hr12oDul0CnzDjI3'\u001B[0m,\n",
       "    \u001B[33mchoices\u001B[0m=\u001B[1m[\u001B[0m\n",
       "        \u001B[1;35mChoice\u001B[0m\u001B[1m(\u001B[0m\n",
       "            \u001B[33mfinish_reason\u001B[0m=\u001B[32m'stop'\u001B[0m,\n",
       "            \u001B[33mindex\u001B[0m=\u001B[1;36m0\u001B[0m,\n",
       "            \u001B[33mlogprobs\u001B[0m=\u001B[3;35mNone\u001B[0m,\n",
       "            \u001B[33mmessage\u001B[0m=\u001B[1;35mChatCompletionMessage\u001B[0m\u001B[1m(\u001B[0m\n",
       "                \u001B[33mcontent\u001B[0m=\u001B[32m'In moonlit gate, swift paws,\\nsteel gleams, a whiskered whisper—\\nvirtue purrs through \u001B[0m\n",
       "\u001B[32mnight.'\u001B[0m,\n",
       "                \u001B[33mrefusal\u001B[0m=\u001B[3;35mNone\u001B[0m,\n",
       "                \u001B[33mrole\u001B[0m=\u001B[32m'assistant'\u001B[0m,\n",
       "                \u001B[33mannotations\u001B[0m=\u001B[1m[\u001B[0m\u001B[1m]\u001B[0m,\n",
       "                \u001B[33maudio\u001B[0m=\u001B[3;35mNone\u001B[0m,\n",
       "                \u001B[33mfunction_call\u001B[0m=\u001B[3;35mNone\u001B[0m,\n",
       "                \u001B[33mtool_calls\u001B[0m=\u001B[3;35mNone\u001B[0m\n",
       "            \u001B[1m)\u001B[0m\n",
       "        \u001B[1m)\u001B[0m\n",
       "    \u001B[1m]\u001B[0m,\n",
       "    \u001B[33mcreated\u001B[0m=\u001B[1;36m1767891169\u001B[0m,\n",
       "    \u001B[33mmodel\u001B[0m=\u001B[32m'gpt-5-nano-2025-08-07'\u001B[0m,\n",
       "    \u001B[33mobject\u001B[0m=\u001B[32m'chat.completion'\u001B[0m,\n",
       "    \u001B[33mservice_tier\u001B[0m=\u001B[32m'default'\u001B[0m,\n",
       "    \u001B[33msystem_fingerprint\u001B[0m=\u001B[3;35mNone\u001B[0m,\n",
       "    \u001B[33musage\u001B[0m=\u001B[1;35mCompletionUsage\u001B[0m\u001B[1m(\u001B[0m\n",
       "        \u001B[33mcompletion_tokens\u001B[0m=\u001B[1;36m34\u001B[0m,\n",
       "        \u001B[33mprompt_tokens\u001B[0m=\u001B[1;36m36\u001B[0m,\n",
       "        \u001B[33mtotal_tokens\u001B[0m=\u001B[1;36m70\u001B[0m,\n",
       "        \u001B[33mcompletion_tokens_details\u001B[0m=\u001B[1;35mCompletionTokensDetails\u001B[0m\u001B[1m(\u001B[0m\n",
       "            \u001B[33maccepted_prediction_tokens\u001B[0m=\u001B[1;36m0\u001B[0m,\n",
       "            \u001B[33maudio_tokens\u001B[0m=\u001B[1;36m0\u001B[0m,\n",
       "            \u001B[33mreasoning_tokens\u001B[0m=\u001B[1;36m0\u001B[0m,\n",
       "            \u001B[33mrejected_prediction_tokens\u001B[0m=\u001B[1;36m0\u001B[0m\n",
       "        \u001B[1m)\u001B[0m,\n",
       "        \u001B[33mprompt_tokens_details\u001B[0m=\u001B[1;35mPromptTokensDetails\u001B[0m\u001B[1m(\u001B[0m\u001B[33maudio_tokens\u001B[0m=\u001B[1;36m0\u001B[0m, \u001B[33mcached_tokens\u001B[0m=\u001B[1;36m0\u001B[0m\u001B[1m)\u001B[0m\n",
       "    \u001B[1m)\u001B[0m\n",
       "\u001B[1m)\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some useful stuff in here, apart from the `content` property, such as the token usage. You might notice some other things too, like `function_call` and `tool_calls`. These are specific to OpenAI models, and not every model supports function calling or tools, so we won't cover them. We can achieve many of the same effects without them anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming a response\n",
    "\n",
    "Streaming a response is mainly for user experience. It allows the user to see the response as it comes in, rather than waiting for the whole response to come in. For many applications, this might not be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the moonlit night,  \n",
      "Silent paws tread soft and swift—  \n",
      "Honor in each leap."
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_query},\n",
    "  ],\n",
    "  max_completion_tokens=128,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this really does is create a streaming object, which acts like a generator. We can then print the chunk as it comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision input\n",
    "A huge draw of OpenAI models is the ability to input vision data. This is useful for a wide range of applications, including:\n",
    "- Image captioning\n",
    "- Object detection\n",
    "- Face recognition\n",
    "- Image generation\n",
    "\n",
    "Let's try an example of inputting an image. First we need to look at the image:\n",
    "\n",
    "![plot](../../../assets/imgs/figure.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the caption from this figure:\n",
    "\n",
    "<table><tr><td>\n",
    "\n",
    "**Fig. 2 Spatial and temporal self-similarity and correlation in switching activity.**\n",
    "\n",
    "_(A) Percolating devices produce complex patterns of switching events that are self-similar in nature. The top panel contains 2400 s of data, with the bottom panels showing segments of the data with 10, 100, and 1000 times greater temporal magnification and with 3, 9, and 27 times greater magnification on the vertical scale (units of G0 = 2e2/h, the quantum of conductance, are used for convenience). The activity patterns appear qualitatively similar on multiple different time scales. (B and E) The probability density function (PDF) for changes in total network conductance, P(ΔG), resulting from switching activity exhibits heavy-tailed probability distributions. (C and F) IEIs follow power law distributions, suggestive of correlations between events. (D and G) Further evidence of temporal correlation between events is given by the autocorrelation function (ACF) of the switching activity (red), which decays as a power law over several decades. When the IEI sequence is shuffled (gray), the correlations between events are destroyed, resulting in a significant increase in slope in the ACF. The data shown in (B) to (D) (sample I) were obtained with our standard (slow) sampling rate, and the data shown in (E) to (G) (sample II) were measured 1000 times faster (see Materials and Methods), providing further evidence for self-similarity._\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure is taken from _[Avalanches and criticality in self-organized nanoscale networks, Mallinson et al., 2019.](https://www.science.org/doi/10.1126/sciadv.aaw8438)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the OpenAI vision model to generate a caption for this figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"This figure is a caption from a paper entitled Avalanches and criticality in self-organized nanoscale networks. \"\n",
    "    \"Please provide a caption for this figure. \"\n",
    "    \"You should describe the figure, grouping the panels where appropriate. \"\n",
    "    \"Feel free to make any inferences you need to.\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of calling a vision model is a little more involved, but OpenAI have a [convenient tutorial](https://platform.openai.com/docs/guides/vision) on how to do this.\n",
    "\n",
    "Essentially we need to first convert the image to a base64 string. We can then pass this to the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"../../../assets/imgs/figure.jpeg\"\n",
    "\n",
    "\n",
    "def get_image_caption(image_path, prompt):\n",
    "  # Getting the base64 string\n",
    "  base64_image = encode_image(image_path)\n",
    "\n",
    "  headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "  }\n",
    "\n",
    "  payload = {\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": prompt\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ],\n",
    "    \"max_completion_tokens\": 512,\n",
    "  }\n",
    "\n",
    "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "  return response.json()['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Caption for Figure:**\n",
      "\n",
      "**Characterization of avalanche dynamics in self-organized nanoscale networks.**\n",
      "\n",
      "**(A)** Temporal fluctuations of the conductance change, ΔG, normalized by a reference value (G₀) across different timescales, from 100 seconds (top panel) to 0.03 seconds (bottom panel). Each panel shows the characteristic noise patterns indicative of self-organized critical behavior.\n",
      "\n",
      "**(B) and (E)** Probability distributions of the magnitude of conductance changes, P(ΔG), plotted against ΔG (in units of G₀). The distributions exhibit power-law behavior with exponents of 2.59 (B) and 2.36 (E).\n",
      "\n",
      "**(C) and (F)** Probability distributions, P(t), showing the distribution of avalanche durations across different timescales. The scaling exponents t are approximately 1.39 (C) and 1.30 (F), highlighting critical behavior.\n",
      "\n",
      "**(D) and (G)** Average avalanche sizes, A(t), as a function of time. Graphs reveal scaling laws with fitting exponents t of 0.19 and 0.23 (D) and 0.64 and 0.66 (G) for different conditions, suggesting diverse dynamical regimes within the nanoscale networks studied. \n",
      "\n",
      "This multifaceted data provides insight into the criticality and self-organization phenomena in nanoscale systems.\n"
     ]
    }
   ],
   "source": [
    "caption = get_image_caption(image_path, prompt)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mean, I don't know about you, but I think that's incredible. Let's consider what it has done:\n",
    "- Correctly grouped the panels in the same way the real caption did.\n",
    "- Provided information on the observation periods.\n",
    "- Drawn out the important information, such as critical exponents.\n",
    "- Made the link between power law distributions and scale-free behaviour.\n",
    "\n",
    "However, it has failed to provide information on temporal correlations, and it has not noticed the self-similarity in caption 1.\n",
    "\n",
    "But this is still quite impressive, and with more information we could potentially get some better captions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "winter-school-llms (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
