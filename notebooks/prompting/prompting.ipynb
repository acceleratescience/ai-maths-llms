{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting can be critical to success.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "- Present two methods for prompting: Chain of Thought (CoT) and Few-shot;\n",
    "- Show you how to create prompt templates using `Jinja2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought (CoT)\n",
    "Chain-of-Thought prompting has a fancy name, but it is actually very simple.\n",
    "\n",
    "We could simply just ask the model to spit out a simple answer to a question. Below are two simple logic problems that we present to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piaget's Glass of Water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = (\n",
    "    \"Suppose I show you two glasses of water, labelled A and B. \"\n",
    "    \"Glasses A and B appear to be identical in shape, and the water level appears \"\n",
    "    \"to be at the same height in both glasses. \"\n",
    "    \"I now bring out a third empty glass, labelled C. \"\n",
    "    \"Glass C appears to be taller and thinner than glasses A and B. \"\n",
    "    \"I pour the water from glass B into glass C. \"\n",
    "    \"The water level in glass C appears to be higher than the water level in glass A. \"\n",
    "    \"Which glass has more water, A or C? \"\n",
    "    \"Keep your answer concise.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classic Piaget Test. Piaget devised a series of tests for children to determine their cognitive development.\n",
    "\n",
    "One of these tests is the prototpyical **conservation** test. Children in the **preoperational stage** of development (ages 2-7) do not have an understanding of matter conservation - they will tend to claim that the taller, thinner glass has more water in it. Children in the **concrete operational stage** (ages 7-11) do have an understanding of matter conservation, and will claim that the two glasses have the same amount of water in them. If you also said that the two glasses have the same amount of water in them, congratulations! You are at least as cognitively developed as a 7-11 year old!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glass A has more water than glass C. The higher water level in glass C is due to the shape of the glass, not the volume of water.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": user_query},\n",
    "  ],\n",
    "  max_completion_tokens=512,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poor `gpt-4o-mini`! It seems to be quite confused by this question - it is _almost_ correct, but not quite. It is not clear why this is the case, but it is clear that the model is not able to perform this task.\n",
    "\n",
    "Try changing the model to `gpt-4o` though, and you will find that it **can** perform this task. Some might argue that this is a type of [emergent ability](https://arxiv.org/abs/2206.07682) - as we start adding more parameters to a model, certain behaviours and abilities unexpectedly emerge. This is a very interesting phenomenon, and it is not necessarily clear [why this happens](https://arxiv.org/abs/2307.15936), or [whether it is actually real](https://arxiv.org/abs/2304.15004) (see a rebuttal to this [here](https://www.jasonwei.net/blog/common-arguments-regarding-emergent-abilities)).\n",
    "\n",
    "If we stick with `gpt-4o-mini`, we can try to prompt the model with a CoT prompt. This is a prompt that is designed to elicit a series of steps from the model whereby it will generate a sort of reasoning process before coming to a conclusion. All we do is add to the end of the prompt,\n",
    "\n",
    "```\n",
    "Carefully think through the problem step by step.\n",
    "```\n",
    "\n",
    "We also make sure to keep the answer concise,\n",
    "\n",
    "```\n",
    "Keep your answer concise.\n",
    "```\n",
    "\n",
    "just to avoid the model generating an overly long-winded response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of water in glasses A and C is the same. When you pour the water from glass B into the taller and thinner glass C, the water level in glass C appears higher due to its shape, but it does not contain more water than what was originally in glass A. Therefore, glass A has the same amount of water as glass C.\n"
     ]
    }
   ],
   "source": [
    "user_query = (\n",
    "    \"Suppose I show you two glasses of water, labelled A and B. \"\n",
    "    \"Glasses A and B appear to be identical in shape, and the water level appears \"\n",
    "    \"to be at the same height in both glasses. \"\n",
    "    \"I now bring out a third empty glass, labelled C. \"\n",
    "    \"Glass C appears to be taller and thinner than glasses A and B. \"\n",
    "    \"I pour the water from glass B into glass C. \"\n",
    "    \"The water level in glass C appears to be higher than the water level in glass A. \"\n",
    "    \"Which glass has more water, A or C? \"\n",
    "    \"Keep your answer concise. Carefully think through the problem step by step.\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": user_query},\n",
    "  ],\n",
    "  max_completion_tokens=512\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This answer is now correct. But before you celeberate, let me show you what happens when you make a seemingly harmless change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = (\n",
    "    \"Suppose I show you two glasses of water, labelled A and B. \"\n",
    "    \"Glasses A and B appear to be identical in shape, and the water level appears \"\n",
    "    \"to be at the same height in both glasses. \"\n",
    "    \"I now bring out a third glass, labelled C. \"\n",
    "    \"Glass C appears to be taller and thinner than glasses A and B. \"\n",
    "    \"I pour the water from glass B into glass C. \"\n",
    "    \"The water level in glass C appears to be higher than the water level in glass A. \"\n",
    "    \"Which glass has more water, A or C? \"\n",
    "    \"Keep your answer concise. Carefully think through the problem step by step.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we have done is remove the condition that glass C is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine which glass has more water, we need to consider the volume of water in each glass.\n",
      "\n",
      "1. **Initial Condition**: Glasses A and B have the same water level, indicating they contain the same volume of water.\n",
      "\n",
      "2. **Pouring Water**: When you pour the water from glass B into glass C, the volume of water from B is transferred to C.\n",
      "\n",
      "3. **Water Levels**: After pouring, the water level in glass C appears higher than in glass A. This is due to the taller and thinner shape of glass C, which can make the water level appear higher even if the volume is not greater.\n",
      "\n",
      "4. **Volume Comparison**: Since glass A and glass B initially had the same volume of water, and you poured all of glass B's water into glass C, glass C now contains the volume of water from glass B plus any water it originally had (if any). \n",
      "\n",
      "5. **Conclusion**: Since glass A has its original volume of water and glass C has at least the volume of water from glass B, glass C must have more water than glass A.\n",
      "\n",
      "Thus, glass C has more water than glass A.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": user_query},\n",
    "  ],\n",
    "  max_completion_tokens=512,\n",
    "  temperature=0.0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is it suddenly now wrong!? We have removed the condition that glass C is empty, and the LLM has \"made a choice\" to assume that there is water in glass C. The LLM cannot read your mind, and ambiguity can work against you!\n",
    "\n",
    "The lesson here is that CoT can be a powerful way to improve performance, but it is **not** foolproof. Hallucination is almost impossible to eliminate, and prompting can be incredibly fragile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now both versions are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Lions Living in Harmony"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is a little harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = (\n",
    "    \"There is an island with infinite grass and vegetation. \"\n",
    "    \"The island is inhabited by 1 sheep and N lions. \"\n",
    "    \"The lions can survive by eating the sheep or vegetation, \"\n",
    "    \"but they much prefer to eat the sheep. \"\n",
    "    \"When a lion eats the sheep, it gets converted into a sheep itself \"\n",
    "    \"and then can in turn be eaten by other lions. \"\n",
    "    \"The lions want to eat the sheep, but not at the risk of being eaten themselves. \"\n",
    "    \"Assume that the lions are perfectly logical and will always make the optimal decision. \"\n",
    "    \"In one sentence, for which number of N will all sheep be safe?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sheep will be safe if there is only 1 lion (N = 1), as the lion will not eat the sheep for fear of being left without food.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": user_query},\n",
    "  ],\n",
    "  max_tokens=1024,\n",
    "  temperature=0.0\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is likely to be incorrect.\n",
    "\n",
    "To be fair, this problem is actually pretty difficult - it is a classic quantitative finance job interview question. Maybe we should not be so hard on `gpt-4o-mini` after all, since most humans don't get this right first time.\n",
    "\n",
    "Now let's add a CoT prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = (\n",
    "    \"There is an island with infinite grass and vegetation. \"\n",
    "    \"The island is inhabited by 1 sheep and N lions. \"\n",
    "    \"The lions can survive by eating the sheep or vegetation, \"\n",
    "    \"but they much prefer to eat the sheep. \"\n",
    "    \"When a lion eats the sheep, it gets converted into a sheep itself \"\n",
    "    \"and then can in turn be eaten by other lions. \"\n",
    "    \"The lions want to eat the sheep, but not at the risk of being eaten themselves. \"\n",
    "    \"Assume that the lions are perfectly logical and will always make the optimal decision. \"\n",
    "    \"For which number of N will all sheep be safe? \"\n",
    "    \"Think through the problem step-by-step.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the problem, let's analyze the scenario step by step based on various values of \\( N \\) (the number of lions).\n",
      "\n",
      "### Case Analysis:\n",
      "\n",
      "1. **N = 1 (1 lion)**:\n",
      "    - The lion will eat the sheep because it does not have to worry about being eaten itself. Once it eats the sheep, it becomes a sheep and there are no sheep left anymore. The sheep is not safe.\n",
      "\n",
      "2. **N = 2 (2 lions)**:\n",
      "    - Each lion wants to eat the sheep, but if one lion eats the sheep, it will become a sheep. The remaining lion could eat that lion (who has now become a sheep), which makes the first lion's position risky. Therefore, neither lion will eat the sheep because eating it would lead to its own death. Thus, the 2 sheep remain alive and safe. \n",
      "\n",
      "3. **N = 3 (3 lions)**:\n",
      "    - Here, each of the three lions has the same thinking as in the 2 lion case. If one lion eats the sheep, the remaining two lions could eat that lion (who has become a sheep). Hence, the logical decision for each lion is not to eat the sheep to avoid becoming vulnerable. The sheep remains safe.\n",
      "\n",
      "4. **N = 4 (4 lions)**:\n",
      "    - Similarly, each lion knows that if it eats the sheep, then one lion will become a sheep, making it possible for the other three to eat that lion. So, none would choose to eat the sheep and thus it remains safe.\n",
      "\n",
      "5. **N = 5 (5 lions)**:\n",
      "    - Following the same logic, if one lion eats the sheep, it turns into a sheep, and that lion would be able to be eaten by the remaining lions, all of whom will avoid risk. Therefore, they will not eat the sheep, and it remains safe.\n",
      "\n",
      "### General Rule:\n",
      "From this analysis, we can see a pattern:\n",
      "\n",
      "- When \\( N \\) is **odd** (such as \\( N = 1, 3, 5, \\ldots \\)), the lions will eat the sheep because the lion that eats it will not be in immediate danger.\n",
      "- When \\( N \\) is **even** (such as \\( N = 2, 4, 6, \\ldots \\)), none of the lions will eat the sheep since they would not want to risk being eaten by another lion after the first lion consumes the sheep.\n",
      "\n",
      "### Conclusion:\n",
      "Therefore, the sheep will be safe when the number of lions \\( N \\) is an **even number**. If \\( N \\) is odd, none of the sheep are safe. So, the answer is that all sheep will be safe when:\n",
      "\n",
      "**N is even (N = 2, 4, 6, ...)**.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": user_query},\n",
    "  ],\n",
    "  max_tokens=1024\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer may or may not be correct. You might have to try it a couple of times to make sure.\n",
    "\n",
    "Just by adding,\n",
    "\n",
    "```\n",
    "Think through the problem step by step.\n",
    "```\n",
    "\n",
    "to the end of the prompt, the model was able to reason its way to the correct answer. Again though, try running it a few times and seeing if it consistently correct. Or try telling the model to be concise...you will likely experience failures.\n",
    "\n",
    "Again CoT prompting can be a powerful technique, but **it will almost certainly fail** at times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot prompting\n",
    "When we want an LLM to do something for us, we could just ask it.\n",
    "\n",
    "In these example, we use a selection of real and fake Haiku and ask the model to classify them as either real or fake. The fake Haiku are generated by Claude, and the real Haiku are from the famous Haiku poets [Matsuo Basho](https://en.wikipedia.org/wiki/Matsuo_Bash%C5%8D), [Yosa Buson](https://en.wikipedia.org/wiki/Yosa_Buson), and [Kobayashi Issa](https://en.wikipedia.org/wiki/Kobayashi_Issa). There are 6 each of the real poets for a total of 18, and 18 fake examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old pond\n",
      "a frog jumps in\n",
      "sound of water\n",
      "\n",
      "Dawn's first light\n",
      "a spider weaves anew\n",
      "its dew-kissed web\n"
     ]
    }
   ],
   "source": [
    "with open(\"haiku/real_haiku_basho.txt\", \"r\") as f:\n",
    "    real_haikus_basho = f.read().split(\"\\n\\n\")\n",
    "\n",
    "with open(\"haiku/real_haiku_buson.txt\", \"r\") as f:\n",
    "    real_haikus_buson = f.read().split(\"\\n\\n\")\n",
    "\n",
    "with open(\"haiku/real_haiku_issa.txt\", \"r\") as f:\n",
    "    real_haikus_issa = f.read().split(\"\\n\\n\")\n",
    "\n",
    "with open(\"haiku/gpt_haiku.txt\", \"r\") as f:\n",
    "    fake_haikus = f.read().split(\"\\n\\n\")\n",
    "\n",
    "print(real_haikus_basho[0])\n",
    "print()\n",
    "print(fake_haikus[0])\n",
    "\n",
    "real_haikus = real_haikus_basho + real_haikus_buson + real_haikus_issa\n",
    "\n",
    "all_haikus = real_haikus + fake_haikus\n",
    "# 1 for real, 0 for fake\n",
    "targets = [1] * len(real_haikus) + [0] * len(fake_haikus)\n",
    "\n",
    "# shuffle\n",
    "import random\n",
    "zipped = list(zip(all_haikus, targets))\n",
    "random.shuffle(zipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the generated Haiku are quite similar to the original Haiku. However, what might throw off the model is that the _real_ Haiku are not necessarily in the 5-7-5 pattern (in fact most of them are not). For a very interesting analysis of influential Japanese Haiku, I strongly recommend Volume 1 of R. H. Blythe's [Haiku: Eastern Culture](https://www.amazon.co.uk/Haiku-Eastern-R-H-Blyth/dp/1621387216).\n",
    "\n",
    "We will have to switch to `gpt-4o` to get the correct answer to this question. `gpt-4o-mini` is not able to perform this task well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:17<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "system_prompt = (\n",
    "    \"You will be shown a haiku that is either written by a human or by a computer. \"\n",
    "    \"Your job is to classify the haiku as either 'human' or 'computer'.\\n\\n\"\n",
    "    \"Classify the following haiku as either written by a human or a computer. \"\n",
    "    \"Respond with 'human' or 'computer' only.\"\n",
    ")\n",
    "\n",
    "result = []\n",
    "for haiku, target in tqdm(zipped):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "          {\"role\": \"system\", \"content\": system_prompt},\n",
    "          {\"role\": \"user\", \"content\": haiku},\n",
    "        ],\n",
    "        max_tokens=128\n",
    "    )\n",
    "    result.append((haiku, target, response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have run the above code using the async client, but with a large group, we could hit rate limits..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(result):\n",
    "    correct = 0\n",
    "    for haiku, target, response in result:\n",
    "        if response == \"human\" and target == 1:\n",
    "            correct += 1\n",
    "        elif response == \"computer\" and target == 0:\n",
    "            correct += 1\n",
    "    return correct / len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.22%\n",
      "1 human\n",
      "0 human\n",
      "0 human\n",
      "1 human\n",
      "1 human\n",
      "0 human\n",
      "0 human\n",
      "1 human\n",
      "0 human\n",
      "0 human\n",
      "0 human\n",
      "1 human\n",
      "1 human\n",
      "0 human\n",
      "1 human\n",
      "1 human\n",
      "0 human\n",
      "0 human\n",
      "0 human\n",
      "0 human\n",
      "0 human\n",
      "1 human\n",
      "0 human\n",
      "1 computer\n",
      "1 human\n",
      "1 human\n",
      "0 human\n",
      "1 human\n",
      "1 human\n",
      "1 human\n",
      "1 human\n",
      "0 human\n",
      "0 human\n",
      "1 human\n",
      "1 human\n",
      "0 human\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy(result)*100:.2f}%')\n",
    "\n",
    "# print target, response\n",
    "for haiku, target, response in result:\n",
    "    print(target, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It thinks almost all of them are human!! This is not what we want. We want the model to be able to distinguish between human and machine generated Haiku.\n",
    "\n",
    "Let's add some examples to the prompt. We could add these into the system prompt itself, but it makes more sense to add them in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"A cicada shell\\nit sang itself\\nutterly away\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"human\"},\n",
    "    {\"role\": \"user\", \"content\": \"Crimson leaves falling\\na path of quiet footsteps\\nautumn fades to dusk\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"computer\"},\n",
    "    {\"role\": \"user\", \"content\": \"It is evening autumn\\nI think only\\nof my parents\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"human\"},\n",
    "    {\"role\": \"user\", \"content\": \"In the moonlit night\\na distant temple bell rings\\nechoes in my heart\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"computer\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:21<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for haiku, target in tqdm(zipped):\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=messages + [{\"role\": \"user\", \"content\": haiku}],\n",
    "      max_tokens=128\n",
    "    )\n",
    "    result.append((haiku, target, response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.11%\n",
      "1 computer\n",
      "0 computer\n",
      "0 computer\n",
      "1 human\n",
      "1 human\n",
      "0 computer\n",
      "0 computer\n",
      "1 human\n",
      "0 computer\n",
      "0 computer\n",
      "0 computer\n",
      "1 human\n",
      "1 human\n",
      "0 computer\n",
      "1 computer\n",
      "1 human\n",
      "0 human\n",
      "0 human\n",
      "0 computer\n",
      "0 human\n",
      "0 computer\n",
      "1 human\n",
      "0 computer\n",
      "1 human\n",
      "1 human\n",
      "1 human\n",
      "0 computer\n",
      "1 human\n",
      "1 human\n",
      "1 human\n",
      "1 human\n",
      "0 computer\n",
      "0 computer\n",
      "1 human\n",
      "1 human\n",
      "0 computer\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy(result)*100:.2f}%')\n",
    "\n",
    "# print target, response\n",
    "for haiku, target, response in result:\n",
    "    print(target, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! So after just showing the model a couple of examples of each, it is now able to distinguish reliably between human and machine-generated Haiku. This is the power of few-shot prompting - if you can show instead of tell, you might end up with better results.\n",
    "\n",
    "It is also possible to combine CoT and few-shot prompting. Just be aware that this may drastically increase the length of the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An aside..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used an LLM for the above task, when we could just have easily used something else. We could have cultivated a broad dataset of real and fake Haiku and fine-tuned a BERT model for this task. More than likely, a fine-tuned BERT would have outperformed an LLM. If we only have a small amount of data, and a small amount of samples to classify an LLM will perform well. However, if we have very many potential training examples, and very many samples to classify, then using an LLM may be very expensive compared to fine-tuning a BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General guidance for prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Be clear and specific\n",
    "The model is not a mind reader. Try and avoid ambiguity in your prompts. If you are asking a question, make sure it is clear what you are asking. If you are asking for a summary, make sure it is clear what you want a summary of. If you want code, make sure you specify language, arguments, returns, etc.\n",
    "\n",
    "#### Be concise\n",
    "LLMs suffer from a retrieval bias, where they tend to favour information at the beginning and ends of prompts. A well known test for LLMs is the **needle in a haystack** test. You take a document and bury an obscure fact or sentence within it. You then ask a question related to this sentence.\n",
    "\n",
    "![needle-in-a-haystack](../../imgs/GPT_4_testing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this effectively shows is that if you make your prompt very long, and the information you are looking for is buried within it, the model may not pay close enough attention to it. Keep important information or instructions at the beginning or end of the prompt, but preferably the end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "winter-school-llms (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
